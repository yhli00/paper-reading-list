# Paper-Reading-list
每周4篇，加油！

**2020-1-23星期日~2020-1-29星期六：**
# Pre-trained Language Models
+ **ELMo**: "Deep contextualized word representations". NAACL(2018) [[pdf]](https://arxiv.org/abs/1802.05365)
+ **GPT**: "Improving Language Understanding by Generative Pre-Training". [[pdf]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
+ **Bert**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". NAACL(2019) [[pdf]](https://arxiv.org/abs/1810.04805)
+ **RoBERTa**: "RoBERTa: A Robustly Optimized BERT Pretraining Approach". arXiv(2019) [[pdf]](https://arxiv.org/abs/1907.11692) [[note]](notes/RoBERTa.md)
+ **ALBERT**: "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations". ICLR(2020) [[pdf]](https://arxiv.org/abs/1909.11942) [[note]](notes/ALBERT.md)
+ **ELECTRA**: "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS". ICLR(2020) [[pdf]](https://arxiv.org/abs/2003.10555) [[note]](notes/ELECTRA.md)
+ **BART**: "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension". ACL(2020) [[pdf]](https://arxiv.org/abs/1910.13461) [[note]](notes/BART.md)

# SLU
+ **RCSF**: "Cross-Domain Slot Filling as Machine Reading Comprehension". IJCAI(2021) [[pdf]](https://www.ijcai.org/proceedings/2021/0550.pdf) [[note]](notes/RCSF.md)
+ **QASF**: "QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining". ACL(2021) [[pdf]](https://aclanthology.org/2021.acl-short.83/) [[note]](notes/QASF.md)
# Others
