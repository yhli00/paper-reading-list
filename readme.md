# Paper-Reading-list
每周4篇，加油！

**2022-1-23星期日~2022-1-29星期六：**
1. [TASE](https://zhuanlan.zhihu.com/p/461651200)
2. [TemplateNER](https://zhuanlan.zhihu.com/p/462088365?)
3. [EntLM](https://zhuanlan.zhihu.com/p/462458103)


# Pre-trained Language Models
+ **ELMo**: "Deep contextualized word representations". NAACL(2018) [[pdf]](https://arxiv.org/abs/1802.05365)
+ **GPT**: "Improving Language Understanding by Generative Pre-Training". [[pdf]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
+ **Bert**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". NAACL(2019) [[pdf]](https://arxiv.org/abs/1810.04805)
+ **RoBERTa**: "RoBERTa: A Robustly Optimized BERT Pretraining Approach". arXiv(2019) [[pdf]](https://arxiv.org/abs/1907.11692) [[note]](notes/RoBERTa.md)
+ **ALBERT**: "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations". ICLR(2020) [[pdf]](https://arxiv.org/abs/1909.11942) [[note]](notes/ALBERT.md)
+ **ELECTRA**: "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS". ICLR(2020) [[pdf]](https://arxiv.org/abs/2003.10555) [[note]](notes/ELECTRA.md)
+ **BART**: "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension". ACL(2020) [[pdf]](https://arxiv.org/abs/1910.13461) [[note]](notes/BART.md)

# SLU
+ **RCSF**: "Cross-Domain Slot Filling as Machine Reading Comprehension". IJCAI(2021) [[pdf]](https://www.ijcai.org/proceedings/2021/0550.pdf) [[note]](notes/RCSF.md)
+ **QASF**: "QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining". ACL(2021) [[pdf]](https://aclanthology.org/2021.acl-short.83/) [[note]](notes/QASF.md)

# MRC
+ **TASE**: "A Simple and Effective Model for Answering Multi-span Questions". EMNLP(2020) [[pdf]](https://arxiv.org/abs/1909.13375) [[note]](https://zhuanlan.zhihu.com/p/461651200)

# NER
+ **TemplateNER**: "Template-Based Named Entity Recognition Using BART". ACL(2021) [[pdf]](https://arxiv.org/pdf/2106.01760.pdf) [[note]](https://zhuanlan.zhihu.com/p/462088365?)
+ **EntLM**: "Template-free Prompt Tuning for Few-shot NER". arXiv(2021) [[pdf]](https://arxiv.org/abs/2109.13532) [[note]](https://zhuanlan.zhihu.com/p/462458103)

# Others
