# Paper-Reading-list
# Pre-trained Language Models
+ **ELMo**: "Deep contextualized word representations". NAACL(2018) [[pdf]](https://arxiv.org/abs/1802.05365)
+ **GPT**: "Improving Language Understanding by Generative Pre-Training". [[pdf]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
+ **Bert**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". NAACL(2019) [[pdf]](https://arxiv.org/abs/1810.04805)
+ **RoBERTa**: "RoBERTa: A Robustly Optimized BERT Pretraining Approach". arXiv(2019) [[pdf]](https://arxiv.org/abs/1907.11692) [[note]](notes/RoBERTa.md)
+ **ALBERT**: "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations". ICLR(2020) [[pdf]](https://arxiv.org/abs/1909.11942) [[note]](notes/ALBERT.md)
# Others
